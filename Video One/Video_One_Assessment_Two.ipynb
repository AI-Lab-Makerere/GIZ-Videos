{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIZ: Video One, Assessment Two.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLIJi2TG3yrW",
        "colab_type": "text"
      },
      "source": [
        "## How to Access Open Voice Training Data: The Mozilla’s Common Voice Platform\n",
        "\n",
        "**Assessment Two**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBm8huY53I51",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "In this notebook, you will use a pre-trained DeepSpeech model to transcribe English speech to text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeqJIRzG35IR",
        "colab_type": "text"
      },
      "source": [
        "### Notebook Configuration\n",
        "In this secion, we shall install all the necessary libraries and files required to build a simple transcriber.\n",
        "1. <a name=\"deepspeech-install\"></a>Install the `deepspeech` library.\n",
        "2. Download the English (en-US) pre-trained `deepspeech` model.\n",
        "3. Download the sample audio files to test the model.\n",
        "4. Test the model on the downloaded audio samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGVc0Wd_34Oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "1e62a393-15af-4c67-d1c0-a15e44ffd933"
      },
      "source": [
        "# 1. Install the deepspeech library\n",
        "!pip install deepspeech\n",
        "\n",
        "# 2. Download the English (en-US) pre-trained DeepSpeech model.\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.0/deepspeech-0.8.0-models.pbmm\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.0/deepspeech-0.8.0-models.tflite\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.0/deepspeech-0.8.0-models.scorer\n",
        "\n",
        "# 3. Import the deepspeech model\n",
        "import deepspeech\n",
        "\n",
        "# 4. Initialize the model with the pretrained weights and the scorer.\n",
        "model_file_path = 'deepspeech-0.8.0-models.pbmm'\n",
        "scorer_file_path = 'deepspeech-0.8.0-models.scorer'\n",
        "model = deepspeech.Model(model_file_path)\n",
        "\n",
        "# Scorer initialization\n",
        "model.enableExternalScorer(scorer_file_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/ff/f17ff70af03d27afb749f866cab2e6f5def29e02d5aa2762afc68ea92eab/deepspeech-0.8.2-cp36-cp36m-manylinux1_x86_64.whl (8.3MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech) (1.18.5)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.8.2\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   652  100   652    0     0   2910      0 --:--:-- --:--:-- --:--:--  2910\n",
            "100  180M  100  180M    0     0  27.7M      0  0:00:06  0:00:06 --:--:-- 34.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   654  100   654    0     0   3070      0 --:--:-- --:--:-- --:--:--  3070\n",
            "100 45.1M  100 45.1M    0     0  17.6M      0  0:00:02  0:00:02 --:--:-- 32.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   654  100   654    0     0   2932      0 --:--:-- --:--:-- --:--:--  2932\n",
            "100  909M  100  909M    0     0  31.8M      0  0:00:28  0:00:28 --:--:-- 29.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL4MZVTJ-awo",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, if you need to use the `deepspeech` version that supports a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit), you could instead use the command below. _(Uncomment the second line before running the cell.)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0jH0-no_O_6",
        "colab_type": "text"
      },
      "source": [
        "Ensure to change the runtime type, from none to GPU. To do this follow the following steps:\n",
        "1. Click on the **Runtime** tab in the notebook top menu.\n",
        "2. Go to and click **Change runtime type**.\n",
        "3. Change from **None** to **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HwuVhBe2EMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. (Alternatively) Install the DeepSpeech Library with GPU support.\n",
        "# !pip install deepspeech-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnHAC0gwAPe9",
        "colab_type": "text"
      },
      "source": [
        "Next, we shall download a pretrained model that we shall use to transcribe the sentence we shall speak from voice (speech) to text.\n",
        "\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- The model with the `.pbmm` extension is memory mapped and thus memory efficient and fast to load. \n",
        "- The model with the `.tflite` extension is converted to use TFLite, has post-training quantization enabled, and is more suitable for resource constrained environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC6-PqYUChqU",
        "colab_type": "text"
      },
      "source": [
        "Next, we shall download audio samples that we shall use to test the pre-trained model. The downloaded files will be unzipped before use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCNBCAoODv6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "81cf67e5-aaf4-40c3-f13e-e3ed9409f7e1"
      },
      "source": [
        "# 3. Download the sample audio files to test the model.\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.0/audio-0.8.0.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   642  100   642    0     0   2853      0 --:--:-- --:--:-- --:--:--  2853\n",
            "100  193k  100  193k    0     0   211k      0 --:--:-- --:--:-- --:--:--  383k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUD0cMm8D6I7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a0d580db-29f3-443c-9665-3563e21b72d0"
      },
      "source": [
        "# Unzip the files.\n",
        "!tar -xvzf audio-0.8.0.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "._audio\n",
            "audio/\n",
            "audio/._2830-3980-0043.wav\n",
            "audio/2830-3980-0043.wav\n",
            "audio/._Attribution.txt\n",
            "audio/Attribution.txt\n",
            "audio/._4507-16021-0012.wav\n",
            "audio/4507-16021-0012.wav\n",
            "audio/._8455-210777-0068.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/._License.txt\n",
            "audio/License.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3g3v7LBacCu",
        "colab_type": "text"
      },
      "source": [
        "Next, we test the pretrained model on the sample audio files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67szRw3nakNf",
        "colab_type": "text"
      },
      "source": [
        "To do this, we shall run the `deepspeech` command, which was made available after the [first step](#deepspeech-install) of the notebook configuration. \n",
        "\n",
        "---\n",
        "Notes:\n",
        "- We run the `deepspeech` command with the `-h` flag to better understand how the command is used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2lioMm0UTkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "856bc6e4-2703-41b1-fbb1-374466893294"
      },
      "source": [
        "!deepspeech -h"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: deepspeech [-h] --model MODEL [--scorer SCORER] --audio AUDIO\n",
            "                  [--beam_width BEAM_WIDTH] [--lm_alpha LM_ALPHA]\n",
            "                  [--lm_beta LM_BETA] [--version] [--extended] [--json]\n",
            "                  [--candidate_transcripts CANDIDATE_TRANSCRIPTS]\n",
            "\n",
            "Running DeepSpeech inference.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         Path to the model (protocol buffer binary file)\n",
            "  --scorer SCORER       Path to the external scorer file\n",
            "  --audio AUDIO         Path to the audio file to run (WAV format)\n",
            "  --beam_width BEAM_WIDTH\n",
            "                        Beam width for the CTC decoder\n",
            "  --lm_alpha LM_ALPHA   Language model weight (lm_alpha). If not specified,\n",
            "                        use default from the scorer package.\n",
            "  --lm_beta LM_BETA     Word insertion bonus (lm_beta). If not specified, use\n",
            "                        default from the scorer package.\n",
            "  --version             Print version and exits\n",
            "  --extended            Output string from extended metadata\n",
            "  --json                Output json from metadata with timestamp of each word\n",
            "  --candidate_transcripts CANDIDATE_TRANSCRIPTS\n",
            "                        Number of candidate transcripts to include in JSON\n",
            "                        output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnF-VASFik-D",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"test-step\"></a>From the output above, `deepspeech` command requires two arguments i.e. `--model` which is the path to the model, and `--audio` which is the path to the audio file to transcribe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AzBjTZVeaed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "ceec7640-85f8-473f-d771-7466e702dd95"
      },
      "source": [
        "# 4. Test the pretrained deepspeech model\n",
        "!deepspeech --model deepspeech-0.8.0-models.pbmm --scorer deepspeech-0.8.0-models.scorer \\\n",
        "            --audio audio/2830-3980-0043.wav\n",
        "\n",
        "# Expirement with the other sample .wav files to ascertain if the transcription is correct."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.8.0-models.pbmm\n",
            "TensorFlow: v2.2.0-24-g1c1b2b9\n",
            "DeepSpeech: v0.8.2-0-g02e4c76\n",
            "2020-08-31 18:07:34.694321: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "Loaded model in 0.0106s.\n",
            "Loading scorer from files deepspeech-0.8.0-models.scorer\n",
            "Loaded scorer in 0.000259s.\n",
            "Running inference.\n",
            "experience proves this\n",
            "Inference took 1.459s for 1.975s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcmpKXASj0CY",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "- You could download one of the `.wav` files from the audio folder to listen to the speech, then compare that with the second last statement from the `deepspeech` command output.\n",
        "- It should be the same, and if it is, then everything is working as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcSqPR6Os1tW",
        "colab_type": "text"
      },
      "source": [
        "### DeepSpeech Model\n",
        "In this section, we shall set up a model that we shall use to transcribe the sentence that we shall speak.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH12Hbn0xI25",
        "colab_type": "text"
      },
      "source": [
        "This is similar to what was done in the [test step](#test-step). At this stage the model is ready to receive and transcribe speech English sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3HjfVMx5Io",
        "colab_type": "text"
      },
      "source": [
        "### A Simple Transcriber\n",
        "A transcriber consists of two main modules, the module that captures voice from a microphone, and the module that converts the voice to text. These modules work simulatenously, the voice capturing module keeps producing chunks of the speech stream while the other module listens to this stream chunks, and converts them to text upon arrival while updating the transcribed text.\n",
        "\n",
        "---\n",
        "**[TO-DO]**<br>\n",
        "To capture your custom sentence;\n",
        "1. You will need to download and install the [Audacity Tool](https://www.fosshub.com/Audacity.html?dwl=audacity-win-2.4.2.exe) _(This URL links directly to the `.exe` file.)_\n",
        "2. When the program has successfully installed, click on the record button &#9210;.\n",
        "3. Proceed to record your sentence(s), when done, click the stop &#9209; button.\n",
        "4. To save the recording, go to the file tab, then export the file in the preferred format `.wav`,`.mp3`. \n",
        "5. Finally, upload the file to this notebook and mark its path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WluNp0ccE8fM",
        "colab_type": "text"
      },
      "source": [
        "### Transcription Process\n",
        "With the voice file uploaded, we shall run it through the model for the transcription."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz-uLzuMFtGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Set the filepath, change 'audio/4507-16021-0012.wav' to the full file path of your custom sentence.\n",
        "filepath = 'audio/4507-16021-0012.wav'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7tuuGHxGnLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title simple_transcriber function (Run Cell) { display-mode: \"form\" }\n",
        "import wave\n",
        "import numpy as np\n",
        "# transcription function\n",
        "def simple_transcriber(filepath):\n",
        "  \"\"\"\n",
        "    This function transcribes the input sentence from voice to text.\n",
        "\n",
        "    filepath <string> path/to/custom_sentence\n",
        "  \"\"\"\n",
        "  w = wave.open(filepath, 'r')\n",
        "  rate = w.getframerate()\n",
        "  frames = w.getnframes()\n",
        "  buffer = w.readframes(frames)\n",
        "\n",
        "  # DeepSpeech expects a 16-bit integer array, but the wave library returns a byte\n",
        "  # array, we use numpy to convert it to the 16-bit format.\n",
        "\n",
        "  data = np.frombuffer(buffer, dtype=np.int16)\n",
        "\n",
        "  # Transcribe\n",
        "  text = model.stt(data)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WsrIG0HIGkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transcription\n",
        "text_from_speech = simple_transcriber(filepath)\n",
        "print(text_from_speech)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eevzt9ZlJpZZ",
        "colab_type": "text"
      },
      "source": [
        "### Alternatively (Optional)\n",
        "Instead of using the Audacity Tool, it is possible to record your custom sentence and get it transcribed in real time.\n",
        "\n",
        "---\n",
        "**NOTE:**<br>\n",
        "This cannot be done on Google Colaboratory, as this environment runs on a virtual machine that does not support a microphone, as such, using this method requires downloading this notebook and running it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6zAHjiZzuwo",
        "colab_type": "text"
      },
      "source": [
        "Steps\n",
        "1. Install the `pyaudio` module, this is a python binding for [PortAudio](http://www.portaudio.com/), an open source audio input and output library. We install `pyaudio` in a two step process.\n",
        "- Install the dependency tools in the environment.\n",
        "- Then using `pip`, install `pyaudio` library.\n",
        "\n",
        "---\n",
        "\n",
        "- `pyaudio` library captures the sentence(s) that you speak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex51y5qo1orV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. Install pyaudio module to capture audio.\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install pyaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46nlKGkFOMVe",
        "colab_type": "text"
      },
      "source": [
        "Run the cell below only on your local computer, it returns an `OSError` in this environment, given it has no access to a microphone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnz6UjG9Kur1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this cell only on your local computer! { display-mode: \"form\" }\n",
        "\n",
        "# Create a Streaming session\n",
        "context = model.createStream()\n",
        "\n",
        "# Encapsulate DeepSpeech audio feeding into a callback for PyAudio\n",
        "text_so_far = ''\n",
        "def process_audio(in_data, frame_count, time_info, status):\n",
        "    global text_so_far\n",
        "    data16 = np.frombuffer(in_data, dtype=np.int16)\n",
        "    model.feedAudioContent(context, data16)\n",
        "    text = model.intermediateDecode(context)\n",
        "    if text != text_so_far:\n",
        "        print('Interim text = {}'.format(text))\n",
        "        text_so_far = text\n",
        "    return (in_data, pyaudio.paContinue)\n",
        "\n",
        "# PyAudio parameters\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "RATE = 16000\n",
        "CHUNK_SIZE = 1024\n",
        "\n",
        "# Feed audio to deepspeech in a callback to PyAudio\n",
        "audio = pyaudio.PyAudio()\n",
        "stream = audio.open(\n",
        "    format=FORMAT,\n",
        "    channels=CHANNELS,\n",
        "    rate=RATE,\n",
        "    input=True,\n",
        "    frames_per_buffer=CHUNK_SIZE,\n",
        "    stream_callback=process_audio\n",
        ")\n",
        "\n",
        "def real_time_transcriber():\n",
        "  print('Please start speaking, when done press Ctrl-C ...')\n",
        "  stream.start_stream()\n",
        "\n",
        "  try: \n",
        "      while stream.is_active():\n",
        "          time.sleep(0.1)\n",
        "  except KeyboardInterrupt:\n",
        "      # PyAudio\n",
        "      stream.stop_stream()\n",
        "      stream.close()\n",
        "      audio.terminate()\n",
        "      print('Finished recording.')\n",
        "      # DeepSpeech\n",
        "      text = model.finishStream(context)\n",
        "      print('Final text = {}'.format(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j3iiueNLwvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell and follow the prompts.\n",
        "real_time_transcriber()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}