{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIZ: Video One, Assessment Three (Optional).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moK8Z-Wjaz2A",
        "colab_type": "text"
      },
      "source": [
        "# How to Access Open Voice Training Data: The Mozillaâ€™s Common Voice Platform\n",
        "\n",
        "**Assessment Three**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTkC4KLKbHnI",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "In this notebook, you will train a [Mozilla Voice STT / DeepSpeech](https://deepspeech.readthedocs.io/en/v0.8.0/) using the Single Word Target Segment dataset.\n",
        "\n",
        "### Learning Objectives\n",
        "1. How to train a simple speech to text model.\n",
        "2. How to evaluate the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzuZdMH7Wd2N",
        "colab_type": "text"
      },
      "source": [
        "### General Steps:\n",
        "1. Configure the notebook.\n",
        "2. Get the training data.\n",
        "3. Prepare the data for training.\n",
        "4. Train and Evaluate the STT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9C7Wd9xVZD1",
        "colab_type": "text"
      },
      "source": [
        "### 1.0 Notebook Configuration\n",
        "We shall  start with the installation of the dependency libraries required for the Speech to Text model.\n",
        "***\n",
        "Steps:\n",
        "1. Clone the [DeepSpeech](https://github.com/mozilla/DeepSpeech) repository from GitHub. We use `git` command to clone the repository.\n",
        "2. Install environment softwares required by the DeepSpeech library including [Sound eXchange](http://sox.sourceforge.net/), which is an audio format conversion library i.e. from `.mp3` to `.wav`.\n",
        "3. Setup the cloned repository by installing the libraries listed in the `setup.py` script. This step installs all the dependency libraries required by `DeepSpeech`.\n",
        "4. `DeepSpeech` library is built on `Tensorflow` version 1.15.2, however this environment uses the latest `Tensorflow` version, it is therefore important to downgarde `Tensorflow` to the right version.\n",
        "5. Finally, pretrained English models are downloaded. The pretrained enable us to leverage transfer learning by training on a small dataset and achieving reasonable results.\n",
        "Run the cell below to run through the steps listed above. If you are curious to understand what exactly is happening, double click on the cell.\n",
        "***\n",
        "**NOTE: The setup takes quite a while to finish, kindly be patient.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZAIuGA8MyHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper libraries\n",
        "import os\n",
        "\n",
        "# 1. Clone the STT repository from GitHub.\n",
        "!git clone https://github.com/mozilla/STT.git\n",
        "\n",
        "# 2. Install the dependency libraries.\n",
        "!apt-get install sox libsox-fmt-mp3\n",
        "!pip install sox\n",
        "!apt-get install python3-dev\n",
        "\n",
        "# 3a. Change into the STT directory\n",
        "%cd STT\n",
        "\n",
        "# 3b. Install the model's required libraries.\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --upgrade -e .\n",
        "\n",
        "# 4. Downgrade the Tensorflow version to 1.15.2, STT uses this version\n",
        "#    of Tensorflow.\n",
        "!pip install tensorflow_gpu==1.15.2\n",
        "\n",
        "# 5. Download a pretrained model into the ckeckpoint directory.\n",
        "# 5a. Download the English (en-US) pre-trained DeepSpeech model.\n",
        "!wget -P checkpoints/ https://github.com/mozilla/STT/releases/download/v0.8.0/deepspeech-0.8.0-checkpoint.tar.gz\n",
        "!wget -P checkpoints/ https://github.com/mozilla/STT/releases/download/v0.8.0/deepspeech-0.8.0-models.scorer\n",
        "\n",
        "# 6. Untar the checkpoints into the checkpoint folder\n",
        "!tar -xvf /content/STT/checkpoints/deepspeech-0.8.0-checkpoint.tar.gz -C /content/STT/checkpoints/\n",
        "\n",
        "# 7. Install git-lfs\n",
        "!apt-get install git-lfs\n",
        "!git lfs pull\n",
        "!git lfs install\n",
        "\n",
        "# 8. In this step we define the utility functions that abstract the training process\n",
        "#    in functions as opposed to calling the commands.\n",
        "def prepare_data(path):\n",
        "  \"\"\"\n",
        "    This function calls the import_cv2.py script that \n",
        "    converts the dataset into the .wav format.\n",
        "\n",
        "    path <string> the path to the dataset directory.\n",
        "  \"\"\"\n",
        "  !bin/import_cv2.py {path}\n",
        "\n",
        "def train_model(train, dev, test, epochs):\n",
        "  \"\"\"\n",
        "    Calls the DeepSpeech script to train on the specified \n",
        "    sets.\n",
        "\n",
        "    train <string> the path to the train files.\n",
        "    dev <string>   the path to the val files.\n",
        "    test <string>  the path to the test files.\n",
        "    epochs <int>   the number of training epochs.\n",
        "  \"\"\"\n",
        "  !python3 DeepSpeech.py \\\n",
        "    --train_files {train} \\\n",
        "    --dev_files {dev} \\\n",
        "    --test_files {test} \\\n",
        "    --epochs {epochs} \\\n",
        "    --scorer_path /content/STT/checkpoints/deepspeech-0.8.0-models.scorer\\\n",
        "    --load_checkpoint_dir /content/STT/checkpoints/deepspeech-0.8.0-checkpoint \\\n",
        "    --save_checkpoint_dir /content/STT/checkpoints/deepspeech-0.8.0-checkpoint \\\n",
        "    --n_hidden 2048 \\\n",
        "    --train_cudnn True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ahjjhXvKU-y",
        "colab_type": "text"
      },
      "source": [
        "### 2.0 Getting the training dataset.\n",
        "For the training, we shall use the Single Word Target Segment dataset, this is a use case driven segment containing data to power spoken digit recognition, yes / no detection, and wakeword testing data for [Firefox Voice](https://voice.mozilla.org/firefox-voice).\n",
        "***\n",
        "Steps:\n",
        "1. Download the dataset from [Common Voice Datasets](https://commonvoice.mozilla.org/en/datasets) using `wget`.\n",
        "2. Uncompress the `tar.gz` file.\n",
        "***\n",
        "**NOTE: This process takes some time (about 3 minutes), kindly be patient as the cell runs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3b587HJM9gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Download the dataset from Common Voice Datasets.\n",
        "!wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-5-singleword/cv-corpus-5-singleword.tar.gz\n",
        "\n",
        "# 2. Uncompress the tar.gz file.\n",
        "!tar -xvf /content/STT/cv-corpus-5-singleword.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4zBKg32ifBH",
        "colab_type": "text"
      },
      "source": [
        "### 3.0 Prepare the data\n",
        "In this section, the dataset will be converted into the audio format `.wav` that the STT model uses. When this process is done, each `.mp3` in the dataset will be converted to a `.wav` file. Under the `clips` folder, three `.csv` files will be created.\n",
        "- `clips\\train.csv`, this file contains the names of the training files.\n",
        "- `clips\\dev.csv`, contains the names of the validation files.\n",
        "- `clips\\test.csv` contains the names of the test files.\n",
        "***\n",
        "**NOTE: This process takes quite a while (about 12 minutes) to finish, kindly be patient.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HpdsdYiikfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the prepare_data function defined in the notebook configuration.\n",
        "prepare_data('/content/STT/cv-corpus-5-singleword/en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loRMyHbDTCaA",
        "colab_type": "text"
      },
      "source": [
        "### 4.0 Train and Evaluate the STT model.\n",
        "With the data prepared, we can now train the model, STT uses the `DeepSpeech.py` as a central training, testing, evaluation and model exporting script. When the specified epochs are done, the function evaluates the trained model on the test set, returning the [`WER`](https://en.wikipedia.org/wiki/Word_error_rate), the [`CER`](https://rechtsprechung-im-ostseeraum.archiv.uni-greifswald.de/word-error-rate-character-error-rate-how-to-evaluate-a-model/#:~:text=The%20Word%20Error%20Rate%20(WER,punctuations%2C%20spaces%2C%20etc.) and the `loss`. <br>\n",
        "The `WER` value is given between 0 (i.e no error) and 1 (i.e. the model didn't recognize any word).\n",
        "***\n",
        "We shall use the `train_model` function defined in the notebook configuration. The function takes four arguments:\n",
        "- `train`, this is the path to the `train.csv` file.\n",
        "- `dev`, this is the path to the `dev.csv` file, this file contains the names of the validation files.\n",
        "- `test`, this is the path to the `test.csv` file.\n",
        "- `epochs`, the number of iterations to train the model on the train set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De3GnGy7VeoN",
        "colab_type": "text"
      },
      "source": [
        "The `train_model` function encapsulates a number of attributes to the `DeepSpeech.py` script, to learn more about these attributes, run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECfJifb884GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Displays the full list of parameters used with the training script,\n",
        "# You could expirement with these flags by adding them to the train_model \n",
        "# function defined in the notebook configuration.\n",
        "!python DeepSpeech.py --helpfull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRraa-ME2gSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The full path to the train.csv file.\n",
        "train = r'/content/STT/cv-corpus-5-singleword/en/clips/train.csv'\n",
        "\n",
        "# The full path to the dev.csv file.\n",
        "dev = r'/content/STT/cv-corpus-5-singleword/en/clips/dev.csv'\n",
        "\n",
        "# The full path to the test.csv.\n",
        "test = r'/content/STT/cv-corpus-5-singleword/en/clips/test.csv'\n",
        "\n",
        "# The number of epochs to train the model, increase the epochs to get better\n",
        "# performance.\n",
        "epochs = 30\n",
        "\n",
        "# Finally, we start the model training process by calling the train_model\n",
        "# function.\n",
        "train_model(train, dev, test, epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG2UPF6OHqfi",
        "colab_type": "text"
      },
      "source": [
        "### Reporting to Atingi.\n",
        "1. Record the best `WER` that you get from the model evaluation, this value that will be reported back to Atingi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVXoBA8r8uAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}